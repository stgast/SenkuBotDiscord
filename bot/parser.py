import aiohttp
from bs4 import BeautifulSoup
from typing import List, Dict
from deep_translator import GoogleTranslator
import re

MAL_NEWS_URL = 'https://myanimelist.net/news'


def fix_image_url(url: str) -> str:
    """–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç —É–º–µ–Ω—å—à–µ–Ω–Ω—ã–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏ MAL –¥–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞"""
    if not url:
        return url
    # –ü—Ä–∏–º–µ—Ä: /r/100x156/s/common/... -> /s/common/...
    if "/r/" in url and "/s/" in url:
        start = url.find("/r/")
        end = url.find("/s/")
        fixed = url[:start] + url[end:]
        return fixed
    return url


def translate_to_ru(text: str) -> str:
    """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–∏–π —Å –ø–æ–º–æ—â—å—é Google Translator"""
    try:
        return GoogleTranslator(source="auto", target="ru").translate(text)
    except Exception as e:
        print("Translation failed:", e)
        return text


async def fetch_page(session: aiohttp.ClientSession, url: str) -> str:
    async with session.get(url, timeout=15) as resp:
        resp.raise_for_status()
        return await resp.text()


async def fetch_full_text(session: aiohttp.ClientSession, url: str) -> str:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    try:
        html = await fetch_page(session, url)
        soup = BeautifulSoup(html, 'html.parser')

        # –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã, –≥–¥–µ MAL —Ö—Ä–∞–Ω–∏—Ç —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏
        selectors = [
            '.content-news',
            '.news-container',
            '.news-container__content',
            '.text-readability',
            '.content',
            '.news-body',
            '.news-text',
            '.article-body',
            '#content',
            '.js-article-body',
            '.entry-content',
        ]

        content = None
        for sel in selectors:
            content = soup.select_one(sel)
            if content:
                break

        if not content:
            # –ø–æ—Å–ª–µ–¥–Ω–∏–π —à–∞–Ω—Å ‚Äî –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å –æ—Å–Ω–æ–≤–Ω—ã–º —Å–æ–¥–µ—Ä–∂–∏–º—ã–º
            content = soup.find('article') or soup.find('div', {'class': 'news'})

        if not content:
            print("‚ö†Ô∏è No content found for:", url)
            return ""

        # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –≤—Å–µ—Ö <p> –≤–Ω—É—Ç—Ä–∏ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –±–ª–æ–∫–∞
        paragraphs = [p.get_text(" ", strip=True) for p in content.find_all('p') if p.get_text(strip=True)]

        # –ï—Å–ª–∏ –µ—Å—Ç—å –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π (–∫–æ—Ä–æ—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ)
        if paragraphs:
            first_para = paragraphs[0].strip()
            # –µ—Å–ª–∏ –µ—Å—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã ‚Äî –¥–æ–±–∞–≤–∏–º –º–Ω–æ–≥–æ—Ç–æ—á–∏–µ –¥–ª—è –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è –æ–±—Ä–µ–∑–∫–∏
            if len(paragraphs) > 1:
                return first_para + "\n\n..."
            return first_para

        # –ï—Å–ª–∏ –Ω–µ—Ç –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ ‚Äî —Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—É—é –ª–æ–≥–∏—á–µ—Å–∫—É—é —á–∞—Å—Ç—å
        full_text = " ".join(list(content.stripped_strings))
        if '\n\n' in full_text:
            return full_text.split('\n\n', 1)[0].strip()
        # —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º –≤ –∫—Ä–∞–π–Ω–µ–º —Å–ª—É—á–∞–µ
        sentences = re.split(r'(?<=[.!?])\s+', full_text)
        return sentences[0].strip() if sentences else full_text.strip()

    except Exception as e:
        print("Error fetching full text:", e)
        return ""


async def parse_latest_news(limit: int = 5) -> List[Dict]:
    """–ü–∞—Ä—Å–∏—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ —Å MyAnimeList"""
    results = []
    print(f"üîç Fetching {limit} latest news from MyAnimeList...")
    async with aiohttp.ClientSession() as session:
        html = await fetch_page(session, MAL_NEWS_URL)
        soup = BeautifulSoup(html, 'html.parser')
        news_units = soup.select('.news-unit')

        for unit in news_units[:limit]:
            a = unit.select_one('p.title a')
            if not a:
                continue

            title = a.text.strip()
            link = a['href']

            # üéØ –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∞–Ω–∏–º–µ –≤ –æ–¥–∏–Ω–∞—Ä–Ω—ã—Ö –∫–∞–≤—ã—á–∫–∞—Ö
            match = re.search(r"'([^']+)'", title)
            anime_name = match.group(1) if match else None

            # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –≤ –∫–∞–≤—ã—á–∫–∞—Ö ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –µ–≥–æ, –∏–Ω–∞—á–µ –ø–µ—Ä–µ–≤–æ–¥–∏–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            if anime_name:
                title = f"„Äé{anime_name}„Äè"
            else:
                title = translate_to_ru(title)

            # –±–µ–∑–æ–ø–∞—Å–Ω–æ –ø–æ–ª—É—á–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            img = None
            img_tag = unit.select_one('img')
            if img_tag:
                img = img_tag.get('data-src') or img_tag.get('src')
                img = fix_image_url(img)

            # —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏ ‚Äî –ø—Ä–æ–±—É–µ–º –≤—ã—Ç—è–Ω—É—Ç—å –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            excerpt_tag = unit.select_one('.text')
            excerpt = excerpt_tag.text.strip() if excerpt_tag else ''
            full_text = await fetch_full_text(session, link)
            excerpt = full_text or excerpt  # –µ—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–æ—Å—å –¥–æ—Å—Ç–∞—Ç—å ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç

            results.append({
                'id': link,
                'title': title,
                'link': link,
                'image': img,
                'excerpt': excerpt,
            })

    # –ø–µ—Ä–µ–≤–æ–¥–∏–º –Ω–∞ —Ä—É—Å—Å–∫–∏–π
    translated_results = []
    for item in results:
        if isinstance(item, dict):
            if item.get("title"):
                item["title"] = item["title"]
            if item.get("excerpt"):
                item["excerpt"] = translate_to_ru(item["excerpt"])
            translated_results.append(item)

    print(f"‚úÖ Parsed {len(results)} news items successfully.")
    return translated_results